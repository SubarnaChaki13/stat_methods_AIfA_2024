{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# P07: Bayesian inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 1: Posterior mean of Gaussian random variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us assume that we make $n$ measurements, $D={x_{1}, x_{2}, ..., x_{n}}$, of a Gaussian random variable with mean $\\mu$ and variance $\\sigma^{2}$ (e.g. weight of a person). Therefore the likelihood for each individual measurement is given by \n",
    "$$P(x_{i}\\vert \\mu, \\sigma) = \\frac{1}{\\sqrt{2 \\pi \\sigma^{2}}}e^{-\\frac{1}{2 \\sigma^{2}}(x_{i}-\\mu)^{2}}.$$ We are interested in the posterior distribution of the mean of this Gaussian random variable. \n",
    "\n",
    "Let us assume that we know the variance $\\sigma^{2}$ e.g. because it is solely determined by measurement noise which is well known. \n",
    "\n",
    "(i) Compute the posterior distribution for the mean $\\mu$ for a known variance $\\sigma^{2}$ using Bayes' Theorem and assuming a Gaussian prior \n",
    "$$\\pi(\\mu) = \\frac{1}{\\sqrt{2 \\pi \\sigma_{0}^{2}}}e^{-\\frac{1}{2 \\sigma_{0}^{2}}(\\mu-\\mu_{0})^{2}},$$ \n",
    "on the mean.\n",
    "\n",
    "(ii) Show and discuss what happens for $\\sigma_0\\to\\infty$.\n",
    "\n",
    "(iii) Show and discuss what happens for $n\\to\\infty$.\n",
    "\n",
    "(iV) Discuss what happens for finite $n$ and $\\sigma\\gg \\sigma_0$.\n",
    "\n",
    "**Hint**: It will be useful to express the joint likelihood of the $n$ measurements in terms of the sample mean $\\bar{x}$: \n",
    "$$\\bar{x} = \\frac{1}{n} \\sum_{i=1}^{n} x_{i},$$ \n",
    "and biased sample variance $s^{2}$ \n",
    "$$s^{2} = \\frac{1}{n} \\sum_{i=1}^{n} (x_{i}-\\bar{x})^{2}.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 2: Posterior mean of Gaussian random variable with unknown variance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us repeat the experiment from problem 1, but this time assuming that we do not know the variance in the measurement a priori. Therefore we would like to estimate both the mean and the variance from the data. We assume a *reference prior* on both mean and variance. This results in a uniform prior on $\\mu$ and a uniform prior on $\\log{\\sigma}$ which leads to the joint prior $$\\pi(\\mu, \\sigma) = \\frac{1}{\\sigma}.$$ \n",
    "(i) Using Bayes' Theorem, determine the posterior of the mean by marginalizing the posterior $p(\\mu, \\sigma \\vert D)$ over $\\sigma$ i.e. $$p(\\mu \\vert D) = \\int p(\\mu, \\sigma \\vert D) d\\sigma.$$\n",
    "(ii) Do you recognize the distribution? What is the difference with our earlier discussion of this distribution?\n",
    "\n",
    "**Note**: A reference prior is a prior with which the contribution of the data to the posterior is maximized. This leads to different priors for location and scale parameters (denoted $\\theta$) of a pdf, which we can understand intuitively:\n",
    "* location parameter (measures the location of the pdf, e.g. mean): if we are ignorant about where to center the pdf, we apply a uniform prior on the real axis, i.e. $\\pi(\\theta) \\propto 1$.\n",
    "* scale parameter (measures the dispersion of the pdf, e.g. variance): if we are ignorant about the dispersion of the pdf, we apply a prior that equally treats each order of magnitude i.e. is uniform in $\\log{\\theta}$; this is equivalent to $\\pi(\\theta) \\propto \\frac{1}{\\theta}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 3: The effect of the prior"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imagine you are performing a coin toss experiment with a friend: essentially, your friend flips a coin $n$ times and you document the outcomes. Based on the outcome after $n$ tosses, you want to estimate the probability of getting a head (i.e. if the coin is fair or not).\n",
    "\n",
    "Let $\\theta$ be the probability of getting a head with a given coin. Then the probability of obtaining $h$ heads when tossing a coin $n$ times is given by the Binomial distribution as $$p(h|\\theta)=\\theta^h(1-\\theta)^{n-h}.$$\n",
    "\n",
    "(i) Let us assume you have made the 1000 observations given in `coin_tosses_1.txt`, where 1 denotes head and 0 denotes tails. Further assume that you trust your friend and assume a flat pior. Use Bayes' theorem to derive the posterior for $\\theta$. Plot the distribution after 10, 50, 100, 500 and 1000 tosses. What do you observe?\n",
    "\n",
    "(ii) You and your friend now repeat the experiment with another coin and obtain the measurements in `coin_tosses_2.txt`. Based on your experience from (i), you assume a Gaussian prior on $\\theta$ centered at 0.5 with a standard deviation of 0.2. Use Bayes' theorem to derive the posterior for $\\theta$. Plot the distribution after 10, 50, 100, 500 and 1000 tosses. What do you observe?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "statmeth_env",
   "language": "python",
   "name": "statmeth_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
